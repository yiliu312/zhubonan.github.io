<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://bonanzhu.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://bonanzhu.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-13T08:37:41+00:00</updated><id>https://bonanzhu.com/feed.xml</id><title type="html">blank</title><subtitle>I am a Professor at School of Aerospace Engineering, Beijing Institute of Technology. Previously, I was a postdoctoral researcher at Department of Chemistry, University College London. I started as an experimental physicist in my undergraduate study in Cambridge, and gradually shifted into computational materials science/chemistry over the years. </subtitle><entry><title type="html">SSH反向隧道使远程计算机连接互联网教程</title><link href="https://bonanzhu.com/blog/2024/ssh-reverse-proxy-usage-chinese/" rel="alternate" type="text/html" title="SSH反向隧道使远程计算机连接互联网教程"/><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://bonanzhu.com/blog/2024/ssh-reverse-proxy-usage-chinese</id><content type="html" xml:base="https://bonanzhu.com/blog/2024/ssh-reverse-proxy-usage-chinese/"><![CDATA[<h1 id="ssh反向隧道远程连接互联网教程">SSH反向隧道远程连接互联网教程</h1> <p>某些情况下远程计算机并没有互联网连接（例如超算登录节点），但是又需要访问其中的一些服务。这时，可以通过SSH反向隧道来访问互联网。</p> <h2 id="前提条件">前提条件</h2> <p>需要有以下条件：</p> <ol> <li>本地和远程电脑都安装了SSH。</li> <li>本地电脑已经设置好了代理服务器。</li> <li>你知道如何在命令行中使用SSH。</li> </ol> <h2 id="步骤">步骤</h2> <h3 id="第一步在本地电脑上建立ssh反向隧道">第一步：在本地电脑上建立SSH反向隧道</h3> <p>在你的本地电脑（Linux环境）上打开命令行或PowerShell（Window10之后的系统已经内置SSH客户端），输入以下命令来建立一个SSH反向隧道：</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh <span class="nt">-R</span> 7890:localhost:7890 username@remote_host
</code></pre></div></div> <p>这里，<code class="language-plaintext highlighter-rouge">username</code>是你远程电脑的用户名，<code class="language-plaintext highlighter-rouge">remote_host</code>是远程电脑的IP地址或者主机名。<code class="language-plaintext highlighter-rouge">localhost:7890</code>是你本地计算机上可以访问的代理服务器的地址和端口，需要根据实际情况进行修改。</p> <p>在Linux环境下，可以通过以下命令测试代理服务器是否运作正常：</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">https_proxy</span><span class="o">=</span>http://localhost:7890 curl https://www.baidu.com
</code></pre></div></div> <p>这个命令的含义是：在远程电脑上打开7890端口，并将所有发往这个端口的数据转发到本地电脑的7890端口（即你的代理服务器）。</p> <p>如果你没有可以使用的HTTP代理服务器，可直接输入：</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh <span class="nt">-R</span> 7890 username@remote_host
</code></pre></div></div> <p>SSH会构建一个SOCKS代理服务器，需要注意的是SOCKS代理不负责DNS解析，在完全没有互联网的环境下使用较为困难。</p> <p>注：可修改远程电脑的7890端口为任意一个未被占用的端口。</p> <h3 id="第二步在远程电脑上设置代理">第二步：在远程电脑上设置代理</h3> <p>现在，你需要在远程电脑上设置代理。首先，使用SSH登录到远程电脑：</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh username@remote_host
</code></pre></div></div> <p>然后，设置环境变量<code class="language-plaintext highlighter-rouge">http_proxy</code>和<code class="language-plaintext highlighter-rouge">https_proxy</code>。这两个环境变量告诉你的应用程序如何连接到代理服务器。在命令行中输入以下命令：</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">http_proxy</span><span class="o">=</span>http://localhost:7890
<span class="nb">export </span><span class="nv">https_proxy</span><span class="o">=</span>http://localhost:7890
</code></pre></div></div> <p>在这里，<code class="language-plaintext highlighter-rouge">localhost:7890</code>指的是远程计算机的7890端口，改端口已经通过SSH反向隧道映射到了本地。 因此，链接该端口将连接到你的本地电脑的代理服务器。</p> <p>如果在没有本地电脑可用HTTP代理服务器的情况下，使用了上述的SOCKS代理方案，则需要设置环境变量为：</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">http_proxy</span><span class="o">=</span>socsk5h://localhost:7890
<span class="nb">export </span><span class="nv">https_proxy</span><span class="o">=</span>socsk5h://localhost:7890
</code></pre></div></div> <p>其中<code class="language-plaintext highlighter-rouge">socsk5h://</code>的意思是使用连接SOCKS代理链接的同时也使用代理服务器进行DNS解析，但是并不是所有的程序都支持这种协议。 相比之下，HTTP代理服务器（<code class="language-plaintext highlighter-rouge">http://</code>）的支持更加好一些。</p> <p>现在，你的远程电脑应该能通过本地电脑的代理服务器连接到互联网了。</p> <p>请注意，这些环境变量的设置只在当前的shell会话中有效。如果你退出shell或者重启电脑，你需要重新设置这些环境变量。</p> <h3 id="第三步测试连接">第三步：测试连接</h3> <p>在远程计算机上，你可以通过尝试访问一个网站来测试你的设置是否正确：</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl https://www.baidu.com
</code></pre></div></div> <p>如果你看到了Baidu的首页HTML，那么恭喜你，你已经成功地通过SSH反向隧道连接到互联网了！</p> <h2 id="注意事项">注意事项</h2> <p>SSH反向隧道是一种强大的工具，但也有一些限制和潜在的安全问题。例如，如果你在远程电脑上打开了一个端口，<strong>那么任何人都可以通过这个端口连接到你的本地电脑</strong>。因此，你应该只在信任的网络中使用SSH反向隧道，并且始终保持你的系统和SSH软件的更新。</p> <p>此外，这种方法依赖于你的本地电脑和远程电脑都能够稳定地运行和连接。如果任何一台电脑的网络连接不稳定，或者电脑关机，那么这种方法可能会失败。</p>]]></content><author><name>Bonan Zhu</name></author><category term="posts"/><category term="posts"/><category term="misc"/><category term="linux"/><category term="ssh"/><summary type="html"><![CDATA[SSH反向隧道远程连接互联网教程]]></summary></entry><entry><title type="html">Access the internet (for good) on login nodes behind firewalls</title><link href="https://bonanzhu.com/blog/2022/internet-for-login-nodes/" rel="alternate" type="text/html" title="Access the internet (for good) on login nodes behind firewalls"/><published>2022-11-19T00:00:00+00:00</published><updated>2022-11-19T00:00:00+00:00</updated><id>https://bonanzhu.com/blog/2022/internet-for-login-nodes</id><content type="html" xml:base="https://bonanzhu.com/blog/2022/internet-for-login-nodes/"><![CDATA[<p>On some computing clusters the login nodes do not have internet access. This is a pain for installing software that requires pulling data from a repository, e.g. Python, Julia and many others. Lucky, there is a simple way to bypass this limitation without hacking through the firewall - SOCKS5 proxy with <code class="language-plaintext highlighter-rouge">ssh</code> <a href="https://man.openbsd.org/ssh">reverse tunnelling</a>. This allows the internet traffic to be diverted to a tunnel established by <code class="language-plaintext highlighter-rouge">ssh</code>, going through the client (local) machine.</p> <p>On the local machine, do:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh -R &lt;port_number&gt; &lt;username&gt;@&lt;hostname&gt;
</code></pre></div></div> <p>On the remote machine, one can set the following environmental variables to tell there is a SOCKS5 proxy available:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>export http_proxy=socks5h://localhost:&lt;port_number&gt;
export https_proxy=socks5h://localhost:&lt;port_number&gt;
</code></pre></div></div> <p>or</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>export http_proxy=socks5://localhost:&lt;port_number&gt;
export https_proxy=socks5://localhost:&lt;port_number&gt;
</code></pre></div></div> <p>Where <code class="language-plaintext highlighter-rouge">&lt;port_number&gt;</code> is just a port that is not in use. The only difference between the two is that the former will also divert domain resolution through SOCKS5. Note that certain programs (or different versions of the same program) may only support one (or none) of the above protocols. I have not extensively tested this, but seems that <code class="language-plaintext highlighter-rouge">wget</code> does not work with SOCKS5 at all, but <code class="language-plaintext highlighter-rouge">curl</code> does. For python <code class="language-plaintext highlighter-rouge">pip</code> should work (depends on the <code class="language-plaintext highlighter-rouge">requests</code> library), <code class="language-plaintext highlighter-rouge">conda</code> should also work, but it may need the environmental variable to be in the upper case… Note that for these two one can also specify the proxy directly in the command line or in the configuration file. For Julia, both the two ways above works with <code class="language-plaintext highlighter-rouge">Pkg</code> as of 1.8.</p>]]></content><author><name>Bonan Zhu</name></author><category term="posts"/><category term="posts"/><category term="misc"/><category term="linux"/><summary type="html"><![CDATA[On some computing clusters the login nodes do not have internet access. This is a pain for installing software that requires pulling data from a repository, e.g. Python, Julia and many others. Lucky, there is a simple way to bypass this limitation without hacking through the firewall - SOCKS5 proxy with ssh reverse tunnelling. This allows the internet traffic to be diverted to a tunnel established by ssh, going through the client (local) machine.]]></summary></entry><entry><title type="html">enabling data compression in AiiDA 2.0</title><link href="https://bonanzhu.com/blog/2022/aiida-2.0-and-compression/" rel="alternate" type="text/html" title="enabling data compression in AiiDA 2.0"/><published>2022-06-09T00:00:00+00:00</published><updated>2022-06-09T00:00:00+00:00</updated><id>https://bonanzhu.com/blog/2022/aiida-2.0-and-compression</id><content type="html" xml:base="https://bonanzhu.com/blog/2022/aiida-2.0-and-compression/"><![CDATA[<p>AiiDA 2.0 introduces the new storage format. Rather than placing files individually shard folders, an object storage is used. It includes storage backend stores the data and allows efficient retrival of the data using the hash computed from its content. By default, the <a href="https://github.com/aiidateam/disk-objectstore">disk-objectstore</a> is used for this. The filenames and folder structures are now stored inside the database instead.</p> <p>This backend stores writes in two ways. First, newly added files are stored as plain files on the disk, with the filename being its hash. This allows fully concurrent writing/reading. The storage can be <em>optimized</em> by concatenating these <em>loose</em> files into a single <em>packed</em> file, and the offsets and lengths for reading the data out is stored in a SQLite database. Optionally, when concatenating, the data stream may be compressed. Compression can lead to significant spacing saving for typical text output of DFT codes. While the <code class="language-plaintext highlighter-rouge">verdi storage maintain</code> command can be used to perform this, it is rather conservative and does not enable compression. To get around this, one can use <code class="language-plaintext highlighter-rouge">dostore optimize</code> to perform this task. Current, <code class="language-plaintext highlighter-rouge">disk-objectstore</code> does not support recompressing packed objects, so one may stuck with many uncompressed streams. In addition, when migrating from AiiDA v1, the migrated data is not compressed either. To force the compression, the following line should be changed in <code class="language-plaintext highlighter-rouge">aiida/storage/psql_dos/migrations/utils/utils.py</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">hashkeys</span> <span class="o">=</span> <span class="n">container</span><span class="p">.</span><span class="nf">add_streamed_objects_to_pack</span><span class="p">(</span><span class="n">streams</span><span class="p">,</span> <span class="n">compress</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">open_streams</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <p>to</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">hashkeys</span> <span class="o">=</span> <span class="n">container</span><span class="p">.</span><span class="nf">add_streamed_objects_to_pack</span><span class="p">(</span><span class="n">streams</span><span class="p">,</span> <span class="n">compress</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">open_streams</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <p>Do note that enabling compression would make the migration slower.</p>]]></content><author><name>Bonan Zhu</name></author><category term="posts"/><category term="posts"/><category term="aiida"/><summary type="html"><![CDATA[AiiDA 2.0 introduces the new storage format. Rather than placing files individually shard folders, an object storage is used. It includes storage backend stores the data and allows efficient retrival of the data using the hash computed from its content. By default, the disk-objectstore is used for this. The filenames and folder structures are now stored inside the database instead.]]></summary></entry><entry><title type="html">DFT parallelisation rule of thumb</title><link href="https://bonanzhu.com/blog/2022/DFT-parallelisation-rule-of-thumb/" rel="alternate" type="text/html" title="DFT parallelisation rule of thumb"/><published>2022-02-15T00:00:00+00:00</published><updated>2022-02-15T00:00:00+00:00</updated><id>https://bonanzhu.com/blog/2022/DFT-parallelisation-rule-of-thumb</id><content type="html" xml:base="https://bonanzhu.com/blog/2022/DFT-parallelisation-rule-of-thumb/"><![CDATA[<p><em>TL.DR for VASP</em></p> <ul> <li>Maximise k point parallelisation whenever possible</li> <li>keep number of bands per band group &gt; 10 for GGA calculation</li> <li>use as little lower plane wave parallelisation as possible for hybrid calculations, but keep bands per band group &gt; 4</li> <li>do not over parallelise by using too many core - you can end in the wrong side of the scaling curve</li> </ul> <p>When done efficiently plane wave density functional theory calculations can be very cost-effective for systems up to a few hundreds of atoms. However, there are things to watch out for and ones needs to understand the how parallelisation works in order to maximise the efficiency.</p> <p>While sensible “defaults” are often described in the documenting or chosen by the code itself, there is one thing that the code cannot choose - the number of MPI processors to be used. Unfortunately, when the calculation is slow, the common reaction is to put more CPUs at work, which, in some cases, can even leads to slower calculation and drastic performance drops.</p> <p>First, let’s think about basic theory of parallelisation. I will not pull out the exact equation here though. For any given program, it is consisted of parallisable and the serial parts. Adding more parallelism will make the former faster (in the ideal case), but leave the latter unchanged. This explains the inevitable drop of the speed-up vs num-of-cpus plot, as the latter becomes more and more the dominate contribution to the run time. In addition, parallelisation also have certain overheads, each processor needs to communicate with others in some ways, and the communication cost may increase with increase number of core. This is the cause of the drop in the efficiency with high core counts in the many case. Even if the communication cost does not scale with the number of cores, its contribution to the “parallelised” part still increases with the number of processors because of time of doing actual work becomes less and less.</p> <p>Second, let’s briefly words how plane wave DFT code are parallelised. Most of the codes are parallelised over MPI, the program that is ran with each MPI process, but directives are included in the code itself to orchestrate communication and memory distribution. The parallelisation is commonly conducted by three levels:</p> <ul> <li>kpoints</li> <li>bands</li> <li>plane wave coefficients</li> </ul> <p>Each level corresponds to one of the indices of the “wavefunction” ($W_{kbc}$) which is a multidimensional array. The kpoint parallelisation is the most efficient one, exploiting the fact that each kpoints are almost independent with each other - they only talk to each other via the electron density (which is not the case any more in hybrid DFT). The band parallelisation is conducted by distributing the bands over certain number of processors, and implementing parallel solvers and algorithms. The plane wave coefficients are involved in FFT, and they can be parallelised and distributed as well. Unlike the other two levels, parallelisation over kpoint does not distribute the memory, each process still receives the fully set of kpoints. This is not the case with band and plane wave parallelisation, however, these two does involve heavy (“all-to-all”) communications, making it more and more difficult to scale well on large core counts. On the other hand, one can expected almost perfect scaling with k-point paralellisation. There are of course many fine details on this topic, and many other part of the code (for example, auxiliary arrays and ionic solvers) can cost memory. The digrams below show the the relationship between these levels of parallelisation.</p> <pre><code class="language-mermaid!">flowchart
subgraph All MPI Ranks

subgraph Kpoints
    KG1
    KG2[...]
end

subgraph Bands
    B1
    B2[...]
end

subgraph G-vectors
        Proc1
        Proc2
        Proc3
end
KG1 --&gt; Bands
B1 --&gt; G-vectors
end
</code></pre> <p>At bit on the terminology - the number of <em>kpoint/band groups</em> means that MPI processes are divided into $N$ groups for this level, and each group may contain $M$ number of processes. Hence, $M \times N$ is the total number of processes for this level. Typically, $N$ must be a divisor of the quantity to be parallelised over. For example, for a 320-process calculation with 4 kpoints and 120 bands, there can be:</p> <ul> <li>4 kpoint groups, each with 80 processes, each group works on 1 kpoint</li> <li>4 band groups inside each kpoint group, each group works on 30 bands</li> <li>inside each band group, the work is further parallelise over the G-vectors</li> </ul> <p>For VASP, this information can be obtained on the top of the <code class="language-plaintext highlighter-rouge">OUTCAR</code>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> running  384 mpi-ranks, with    1 threads/rank
 distrk:  each k-point on   32 cores,   12 groups
 distr:  one band on NCORE=   8 cores,    4 groups
</code></pre></div></div> <p>means there there are 12 kpoint groups (32 processes each), 4 band groups (8 processes each). This this calculation there are 48 band, hence each band group has 12 bands to work on. Further down the file:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> k-point  19 :   0.4000 0.4000 0.0000  plane waves:   21064
 k-point  20 :   0.5000 0.4000 0.0000  plane waves:   21070
 k-point  21 :   0.5000 0.5000 0.0000  plane waves:   21084

 maximum and minimum number of plane-waves per node :      2702     2616

</code></pre></div></div> <p>Shows that each processor inside the band group gets about 2700 plane-waves (G-vectors) to work on. If <code class="language-plaintext highlighter-rouge">NCORE</code> is increased, this value will reduce. When there are too few plane waves to be parallelised, the overall efficiency will drop sharply. However, increasing <code class="language-plaintext highlighter-rouge">NCORE</code> also increases the number of bands each band group works on, so improving the parallel efficiency over bands.</p> <p>To summarise, one should maximise the K-point parallelisation, and balance band and G-vector parallelisation by tuning the <code class="language-plaintext highlighter-rouge">NCORE</code> parameter. If one is in a regime that both band and G-vector parallelisation have similar efficiency, choosing <code class="language-plaintext highlighter-rouge">NCORE</code> roughly a square root of the number of processors per group would be optimum. For relatively small GGA calculations, setting <code class="language-plaintext highlighter-rouge">NCORE</code> to be equal to the number of cores in a <a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">NUMA region</a> can be a sensible choice. In those cases, further optimisation of <code class="language-plaintext highlighter-rouge">NCORE</code> may only provide limited gain in speed.</p> <h2 id="hybrid-functional-calculations">Hybrid functional calculations</h2> <p>Hybrid functional calculations often requires a large amount of computing resources to be used for a single calculation to achieve reasonable time to solution (e.g. within a few days). While these calculations are easier to scale purely as a result of them being very compute-heavy (they scale to thousands of cores, while most GGA calculation won’t), careful tuning of parallelisation becomes even more important as every bit of performance gain can leads to huge saving of resources. Thus, one should carefully perform timing analysis for these calculations, typically using the <em>debug</em> or <em>short</em> queue of the supercomputer and run only for a few SCF cycles.</p> <p>In those cases, I would:</p> <ul> <li>parallelise over K points as much as possible if memory permits</li> <li>start from a low NCORE given that the number of bands per band group is more than 4</li> <li>to solve memory issue, reduce the kpoints before switching to “under-populate” the nodes.</li> </ul> <p>Because of the sheer among of resources needed for hybrid DFT, it is very easy to over-parallelise calculations and waste resources. So <em>test, test, test</em>…..</p> <p>Example tests over <code class="language-plaintext highlighter-rouge">NCORE</code> for a 65-atom CdTe defect supercell calculation with 8 kpoints (<code class="language-plaintext highlighter-rouge">KPAR=8</code>), 346 bands (minimum), 47528 plane waves using HSE06 (<code class="language-plaintext highlighter-rouge">ALGO=normal</code>) with VASP6 on <a href="https://www.archer2.ac.uk">ARCHER2</a>:</p> <p><img src="/assets/img/CdTe_Int.png" alt="CdTe defect calculation"/></p> <p>It can be seen that the best performance is achieved with <code class="language-plaintext highlighter-rouge">NCORE=4</code> note that this calculation may actually involve more bands than <code class="language-plaintext highlighter-rouge">NCORE=8</code> and <code class="language-plaintext highlighter-rouge">NCORE=16</code>. Using <code class="language-plaintext highlighter-rouge">NCORE=1</code> or <code class="language-plaintext highlighter-rouge">NCORE=2</code> resulted in very bad performance, probably because there were only one or two bands per band group, and note that the speed up from 1280 to 2560 cores is even worse. <code class="language-plaintext highlighter-rouge">NCORE=4</code> result in about 5 bands per band group (of 80 cores) for the 2560-core calculation and 9 bands per group (of 40 cores) for the 1280-core calculation. Also note the speed up by doubling the core-count - we achieve a 80% gain in speed by throwing twice the computing resource - not bad at all.</p> <h2 id="further-topics">Further topics</h2> <p>We haven not touched OpenMP parallelisation and GPU parallelisation. I have seen good performance with the latter on latest generation of GPUs (e.g. Nvidia A100) for compute heavy hybrid functional VASP calculations. The OpenMP parallelisation can often be used to “recycle” idle cores on underpopulated nodes.</p> <p>We have not touch parallelisation in CASTEP, which is often planed by the code itself rather than the user. Nevertheless, manual tuning can be valuable for large-scale calculations to achieve the good performance.</p>]]></content><author><name>Bonan Zhu</name></author><category term="posts"/><category term="posts"/><summary type="html"><![CDATA[TL.DR for VASP]]></summary></entry><entry><title type="html">Parallel efficiencies of (small) plane wave DFT calculations</title><link href="https://bonanzhu.com/blog/2021/parallel-efficiency-of-pw-DFT/" rel="alternate" type="text/html" title="Parallel efficiencies of (small) plane wave DFT calculations"/><published>2021-07-14T00:00:00+00:00</published><updated>2021-07-14T00:00:00+00:00</updated><id>https://bonanzhu.com/blog/2021/parallel-efficiency-of-pw-DFT</id><content type="html" xml:base="https://bonanzhu.com/blog/2021/parallel-efficiency-of-pw-DFT/"><![CDATA[<p>Plane wave DFT calculations are often known as “cubic-scaling” where the cost grows as the number of atoms (more precisely the number of electrons) cubed. Thankfully, they can be parallelised over many (possibly very large number of cores) to accelerate the calculations. This is often done at multiple levels: the plane wave coefficients, the bands and the k points.</p> <p>A frequent question one may ask when running calculations is: how many cores should I use? Obviously, using more cores <em>should</em> make the calculation faster, but it also increases the time spend on inter-process communications, causing the parallel efficiency to drop with increasing core counts. It not uncommon to see examples where a single calculation can be parallelised over thousands of cores for a supercomputer with relatively small drop on the efficiency. However, the rate of reduction in the parallel efficiency is also highly dependent on the size of the system: those of hard problems (e.g. more atoms) drop much slower than simple and small systems.</p> <p>If there are many calculations to run through, rather than getting the result of each calculation quickly, it can be more efficient to run multiple calculations in parallel, each using a smaller number cores to achieve a higher throughput.</p> <p>Below are some test results for a 28-atom structure using the code <a href="http://www.castep.org">CASTEP</a> with increasing number of cores while maintaining a full population of single compute node with 24 cores. This is a “small” calculation with only 340 eV plane wave cut off energy and 4 kpoints, and 106 bands.</p> <p><img src="/assets/img/throughput-castep-full-occ.png" alt="Test result"/></p> <p>As one may expect, the parallelisation efficiency drops with increasing number of cores. By running 24 cores jobs instead of 1 cores, the overall throughput has dropped to about 80 %, while running six 4-core jobs has only a 5% lost in efficiency. While running many “small” jobs does increase the overall throughput, it comes at the cost of very long “time-to-result” for each calculation. In reality, computing clusters often have cap of run times, and having long-running time for each calculation risks having unfinished ones getting killed, wasting a certain amount of time and resources. This also means that the aftermath of having any kind of node failure will be higher, as calculations have longer turnaround.</p> <p>One important note is that if the node is not fully occupied in the tests, the benefit of running smaller but and many jobs can be significantly exaggerated, as shown in the plot below.</p> <p><img src="/assets/img/throughput-castep-partial-occ.png" alt="Test result with partial occupation"/></p> <p>Several factors could cause this. First, under populating the node means each MPI process has access to more memory and communication bandwidth. Second, modern CPUs often <a href="https://en.wikipedia.org/wiki/Intel_Turbo_Boost">boosts the frequencies</a> when only a few cores are active to be able to utilise the thermal headroom left by the idling ones.</p>]]></content><author><name>Bonan Zhu</name></author><category term="posts"/><category term="DFT"/><category term="misc"/><category term="posts"/><summary type="html"><![CDATA[Plane wave DFT calculations are often known as “cubic-scaling” where the cost grows as the number of atoms (more precisely the number of electrons) cubed. Thankfully, they can be parallelised over many (possibly very large number of cores) to accelerate the calculations. This is often done at multiple levels: the plane wave coefficients, the bands and the k points.]]></summary></entry><entry><title type="html">Hello World!</title><link href="https://bonanzhu.com/blog/2021/hello-world/" rel="alternate" type="text/html" title="Hello World!"/><published>2021-07-13T00:00:00+00:00</published><updated>2021-07-13T00:00:00+00:00</updated><id>https://bonanzhu.com/blog/2021/hello-world</id><content type="html" xml:base="https://bonanzhu.com/blog/2021/hello-world/"><![CDATA[<p>This my first post using Jekyll - so hello world. The aim of this site is to provide a personal space for sharing my research and also act as a blog. From time to time, I will post contents related to computational materials science and programming. Hopefully, they will be of the interests of my fellow researchers and peers!</p>]]></content><author><name>Bonan Zhu</name></author><category term="posts"/><category term="posts"/><category term="misc"/><summary type="html"><![CDATA[This my first post using Jekyll - so hello world. The aim of this site is to provide a personal space for sharing my research and also act as a blog. From time to time, I will post contents related to computational materials science and programming. Hopefully, they will be of the interests of my fellow researchers and peers!]]></summary></entry></feed>